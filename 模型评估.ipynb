{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本次采用sklearn库作为代码描述，使用sklearn可以简单方便的做出结果\n",
    "# 分析sklearn可以使用其中的指标模块 sklearn.metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics 中的 用户自己可以设定评分模型\n",
    "# Model Selection Interface  模型接口选择器\n",
    "# 1、从用户选项中决定积分器\n",
    "# check_scoring(estimator, scoring=None, allow_none=False)\n",
    "from sklearn.metrics import check_scoring\n",
    "#metrics.check_scoring(estimator, scoring=None, allow_none=False)\n",
    "\n",
    "# 2 从字符串中获取模型的分析器 如“accuracy”, \"f1\", \"roc_auc\"\n",
    "from sklearn.metrics import get_scorer\n",
    "#get_scorer(\"f1\") 等 返回的是 make_scorer(f2, ...)\n",
    "\n",
    "# 3、从根据绩效指标或损失函数确定得分模型\n",
    "from sklearn.metrics import make_scorer\n",
    "# other_scorer = make_scorer(score_func, \n",
    "#                greater_is_better=True, \n",
    "#                 needs_proba=False, \n",
    "#                 needs_threshold=False, **kwargs)\n",
    "# 使用的make_scorer()可以自己调整评估模型，将生产的新模型other_scorer \n",
    "# 可以作为评估模型的参数。 如下面的例子\n",
    "# from sklearn.metrics import fbeta_score, make_scorer\n",
    "# ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "# ftwo_scorer\n",
    "# make_scorer(fbeta_score, beta=2)\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import LinearSVC\n",
    "# grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
    "#                     scoring=ftwo_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics 中的 \n",
    "# Classification metrics 分类指标\n",
    "# 1、分类 精确度\n",
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
    "# Examples\n",
    "# >>> from sklearn.metrics import accuracy_score\n",
    "# >>> y_pred = [0, 2, 1, 3]\n",
    "# >>> y_true = [0, 1, 2, 3]\n",
    "# >>> accuracy_score(y_true, y_pred)\n",
    "# 0.5\n",
    "# >>> accuracy_score(y_true, y_pred, normalize=False)\n",
    "# 2\n",
    "\n",
    "# 2、使用梯形法则计算曲线下面积（AUC） ROC下面的面积\n",
    "from sklearn.metrics import auc\n",
    "# auc(x, y) 需要配合其他函数一起使用如 roc_curve 得到假正率FPR 和真正率TPR\n",
    "# return acc // acc: float\n",
    "# 如下例子\n",
    "# >>> import numpy as np\n",
    "# >>> from sklearn import metrics\n",
    "# >>> y = np.array([1, 1, 2, 2])\n",
    "# >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "# >>> metrics.auc(fpr, tpr)\n",
    "# 0.75\n",
    "\n",
    "# 3、根据预测分数计算平均精度（AP）\n",
    "from sklearn.metrics import average_precision_score\n",
    "# average_precision_score(y_true, y_score, \n",
    "#                 average='macro', pos_label=1, sample_weight=None\n",
    "# Returns average_precision: float \n",
    "# average 有四个参数micro’, ‘macro’ (default), ‘samples’, ‘weighted’\n",
    "# >>> import numpy as np\n",
    "# >>> from sklearn.metrics import average_precision_score\n",
    "# >>> y_true = np.array([0, 0, 1, 1])\n",
    "# >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# >>> average_precision_score(y_true, y_scores)\n",
    "# 0.83...\n",
    "\n",
    "# 4、计算平衡精度\n",
    "# 二元和多类分类问题中的平衡精度可以处理不平衡的数据集。 \n",
    "# 它定义为每个分类获得的召回率的平均值。\n",
    "# from sklearn.metrics import balanced_accuracy_score\n",
    "# balanced_accuracy_score(y_true, y_pred, \n",
    "#                         sample_weight=None, \n",
    "#                         adjusted=False)[source]\n",
    "# Returns balanced_accuracy:float\n",
    "# >>> from sklearn.metrics import balanced_accuracy_score\n",
    "# >>> y_true = [0, 1, 0, 0, 1, 0]\n",
    "# >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
    "# >>> balanced_accuracy_score(y_true, y_pred)\n",
    "# 0.625\n",
    "\n",
    "# 5、计算Brier分数。\n",
    "# 计算Brier分数。 Brier分数越小越好，因此命名为“ loss”。\n",
    "# 在一组N个预测中的所有项目上，Brier分数测量（1）分配给项目i可能结果的预测概率与（2）实际结果之间的均方差。\n",
    "# 因此，一组预测的Brier分数越低，则对预测的校准就越好。\n",
    "# 请注意，Brier分数始终取0到1之间的值，因为这是预测概率（必须在0到1之间）和实际结果（只能取0到1）之间的最大可能差值。 ）。\n",
    "# 布里尔损失由精制损失和校准损失组成。 Brier分数适用于可以构造为真或假的二进制和绝对结果，\n",
    "# 但不适用于可以采用三个或更多值的序数变量（这是因为Brier分数假定所有可能的结果均等价于“遥远”彼此之间）。\n",
    "# 哪个标签被视为肯定标签是通过参数pos_label控制的，该参数的默认值为1。\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# brier_score_loss(y_true, y_prob, \n",
    "#                  sample_weight=None, \n",
    "#                  pos_label=None)\n",
    "# Returns score: float\n",
    "# >>> import numpy as np\n",
    "# >>> from sklearn.metrics import brier_score_loss\n",
    "# >>> y_true = np.array([0, 1, 1, 0])\n",
    "# >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
    "# >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
    "# >>> brier_score_loss(y_true, y_prob)\n",
    "# 0.037...\n",
    "# >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
    "# 0.037...\n",
    "# >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
    "# 0.037...\n",
    "# >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
    "# 0.0\n",
    "\n",
    "#6、建立文字报告，显示主要的分类指标\n",
    "from sklearn.metrics import classification_report\n",
    "# classification_report(y_true, y_pred, \n",
    "#                       labels=None, \n",
    "#                       target_names=None, \n",
    "#                       sample_weight=None, \n",
    "#                       digits=2, \n",
    "#                       output_dict=False, \n",
    "#                       zero_division='warn')\n",
    "# >>> from sklearn.metrics import classification_report\n",
    "# >>> y_true = [0, 1, 2, 2, 2]\n",
    "# >>> y_pred = [0, 0, 2, 2, 1]\n",
    "# >>> target_names = ['class 0', 'class 1', 'class 2']\n",
    "# >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "#               precision    recall  f1-score   support\n",
    "# <BLANKLINE>\n",
    "#      class 0       0.50      1.00      0.67         1\n",
    "#      class 1       0.00      0.00      0.00         1\n",
    "#      class 2       1.00      0.67      0.80         3\n",
    "# <BLANKLINE>\n",
    "#     accuracy                           0.60         5\n",
    "#    macro avg       0.50      0.56      0.49         5\n",
    "# weighted avg       0.70      0.60      0.61         5\n",
    "# <BLANKLINE>\n",
    "# >>> y_pred = [1, 1, 0]\n",
    "# >>> y_true = [1, 1, 1]\n",
    "# >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
    "#               precision    recall  f1-score   support\n",
    "# <BLANKLINE>\n",
    "#            1       1.00      0.67      0.80         3\n",
    "#            2       0.00      0.00      0.00         0\n",
    "#            3       0.00      0.00      0.00         0\n",
    "# <BLANKLINE>\n",
    "#    micro avg       1.00      0.67      0.80         3\n",
    "#    macro avg       0.33      0.22      0.27         3\n",
    "# weighted avg       1.00      0.67      0.80         3\n",
    "# <BLANKLINE>\n",
    "\n",
    "# 7、科恩的kappa：一种用于统计注释者之间协议的统计数据。\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "# cohen_kappa_score(y1, y2, \n",
    "#                   labels=None, \n",
    "#                   weights=None, \n",
    "#                   sample_weight=None)\n",
    "\n",
    "# Returns kappa: float\n",
    "\n",
    "# 8、计算混淆矩阵以评估分类的准确性。\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y_true, y_pred, \n",
    "#                  labels=None, \n",
    "#                  sample_weight=None, \n",
    "#                  normalize=None)\n",
    "# >>> from sklearn.metrics import confusion_matrix\n",
    "# >>> y_true = [2, 0, 2, 2, 0, 1]\n",
    "# >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
    "# >>> confusion_matrix(y_true, y_pred)\n",
    "# array([[2, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [1, 0, 2]])\n",
    "# >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "# >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "# >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "# array([[2, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [1, 0, 2]])\n",
    "\n",
    "# 9、计算折现累积收益。\n",
    "# from sklearn.metrics import dcg_score\n",
    "# dcg_score(y_true, y_score, k=None, \n",
    "#           log_base=2, \n",
    "#           sample_weight=None, \n",
    "#           ignore_ties=False)\n",
    "# Returns discounted_cumulative_gain:float\n",
    "# >>> from sklearn.metrics import dcg_score\n",
    "# >>> # we have groud-truth relevance of some answers to a query:\n",
    "# >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
    "# >>> # we predict scores for the answers\n",
    "# >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
    "# >>> dcg_score(true_relevance, scores) # doctest: +ELLIPSIS\n",
    "# 9.49...\n",
    "# >>> # we can set k to truncate the sum; only top k answers contribute\n",
    "# >>> dcg_score(true_relevance, scores, k=2) # doctest: +ELLIPSIS\n",
    "# 5.63...\n",
    "# >>> # now we have some ties in our prediction\n",
    "# >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
    "# >>> # by default ties are averaged, so here we get the average true\n",
    "# >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
    "# >>> dcg_score(true_relevance, scores, k=1) # doctest: +ELLIPSIS\n",
    "# 7.5\n",
    "# >>> # we can choose to ignore ties for faster results, but only\n",
    "# >>> # if we know there aren't ties in our scores, otherwise we get\n",
    "# >>> # wrong results:\n",
    "# >>> dcg_score(true_relevance,\n",
    "# ...           scores, k=1, ignore_ties=True) # doctest: +ELLIPSIS\n",
    "# 5.0\n",
    "\n",
    "# 10、计算F1分数，也称为平衡F分数或F测量\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_true, y_pred, \n",
    "#          labels=None, \n",
    "#          pos_label=1, \n",
    "#          average='binary', \n",
    "#          sample_weight=None, \n",
    "#          zero_division='warn')\n",
    "# # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# >>> from sklearn.metrics import f1_score\n",
    "# >>> y_true = [0, 1, 2, 0, 1, 2]\n",
    "# >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
    "# >>> f1_score(y_true, y_pred, average='macro')\n",
    "# 0.26...\n",
    "# >>> f1_score(y_true, y_pred, average='micro')\n",
    "# 0.33...\n",
    "# >>> f1_score(y_true, y_pred, average='weighted')\n",
    "# 0.26...\n",
    "# >>> f1_score(y_true, y_pred, average=None)\n",
    "# array([0.8, 0. , 0. ])\n",
    "# >>> y_true = [0, 0, 0, 0, 0, 0]\n",
    "# >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
    "# >>> f1_score(y_true, y_pred, zero_division=1)\n",
    "# 1.0...\n",
    "\n",
    "# ..... 等几天之后\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics 回归评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel ranking metrics 多标签排名指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering metrics 聚类指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biclustering metrics 分类指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise metrics 成对指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
